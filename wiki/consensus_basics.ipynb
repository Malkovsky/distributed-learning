{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consensus basics\n",
    "A basic linear average consensus is a distributed decentralized algorithm that calculates the average of several numbers each stored on a separate computation node. Suppose that \n",
    "* $x_i(0)$ is the initial value stored at node $i$ and $x(0)=(x_1(0), \\ldots, x_n(0))^T$\n",
    "* Let $\\mathcal{N}(i)$ be the set of all neighbours of $i$ which means $i$ has i direct communication with nodes in $\\mathcal{N}(i)$ and can view the values stored in these nodes\n",
    "\n",
    "Consensus protocol is a linear a protocol\n",
    "$$\n",
    "\\DeclareMathOperator*\\diag{diag}\n",
    "x_i(k+1)=x_i(k)+\\gamma\\sum_{j\\in \\mathcal{N}(i)}a_{ij}(x_j(k)-x_i(k))\n",
    "$$\n",
    "or if we denote by $A=(a_{ij})$ the adjacency matrix and by $\\mathcal{D}=\\diag\\{d_1, \\ldots, d_n\\}$ with $d_i=\\sum_{j\\in\\mathcal{N}(i)}a_{ij}$ and with <i>laplacian</i> $\\mathcal{L}=\\mathcal{D}-A$ the process above can be rewritten in matrix form\n",
    "$$\n",
    "x(k+1)=x(k)-\\gamma\\mathcal{L}x(k)=(I-\\gamma \\mathcal{L})x(k).\\tag{1}\n",
    "$$\n",
    "Note that row sum of $\\mathcal{0}$ is zero and so the matrix $I-\\gamma\\mathcal{L}$ is stochastic (its row sum is one), it also applies vice versa: for any stochastic matrix $S$ the matrix $I-S$ is the laplacian of some graph.\n",
    "\n",
    "For a strongly connected graph due to [Perron-Frobenius theorem](https://en.wikipedia.org/wiki/Perron%E2%80%93Frobenius_theorem#Perron%E2%80%93Frobenius_theorem_for_irreducible_non-negative_matrices) for irreducible matrices $x(k)$ converges to $\\frac{wv^T}{w^tv}x(0)$ where $w, v$ are left and right eigenvectors of $\\mathcal{L}$ for an eigenvalue of $0$. Right eigenvector $v$ is always of a form $\\alpha\\mathbb{1}$, that is all components are the same.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average consensus\n",
    "\n",
    "Now lets look at the sum of the components of $x$:\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\sum_{i=1}^nx_i(k+1)&=\\sum_{i=1}^nx_i(k)+\\sum_{i=1}^n\\gamma\\sum_{j\\in \\mathcal{N}(i)}a_{ij}(x_j(k)-x_i(k)) \\\\\n",
    "&=\\sum_{i=1}^nx_i(k)+\\gamma\\sum_{i=1}^nx_i(k)\\sum_{j\\in \\mathcal{N}(i)}(a_{ij}-a_{ji})\n",
    "\\end{array}\n",
    "$$\n",
    "From this we can easily deduce that if the value $\\sum_{j\\in \\mathcal{N}(i)}(a_{ij}-a_{ji})$ is zero for any node $i$ then the sum of components of $x(k)$ does not change and thus\n",
    "$$\n",
    "\\sum_{i=1}^nx_i(k)=\\sum_{i=1}^nx_i(0).\n",
    "$$\n",
    "Moreover is $x(k)\\rightarrow x^*$ then due to the fact that $x^*_i=x^*_j$ we have\n",
    "$$\n",
    "x^*_i=\\frac{1}{n}\\sum_{i=1}^nx_i(0).\n",
    "$$\n",
    "In other words in this case all the components converge to an arithmetic average of initial components, this is usually referred to as <i>average consensus</i>. The graphs that satisfy\n",
    "$$\n",
    "\\sum_{j\\in \\mathcal{N}(i)}(a_{ij}-a_{ji})=0\n",
    "$$\n",
    "are usually called <i>balanced</i> and the corresponding matrix $I-\\gamma\\mathcal{L}$ is called <i>doubly stocahstic</i>. The easiest way to satisfy that property is to set $a_{ij}=a_{ji}$ which is only possible in undirected graphs. In general, balanced non-zero positive weight assignment possible only for strongly connected graphs. It is also notable that for a balanced graph left zero eigenvector of $\\mathcal{L}$ is also of a form $\\alpha\\mathbb{1}$ and thus we have\n",
    "$$\n",
    "x^*=\\frac{1}{n}\\mathbb{1}\\mathbb{1}^Tx(0)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weighted consensus\n",
    "Now suppose we want to find a weighted average in our consensus rather then arithmetic mean. Consider a new variables\n",
    "$$\n",
    "y(k)=Bx(0)\n",
    "$$\n",
    "where $B=\\diag\\{\\beta_1, \\ldots, \\beta_n\\}$. Performing average consensus on $y$ gives us\n",
    "$$\n",
    "y^*=\\frac{1}{n}\\mathbb{1}\\mathbb{1}^Ty(0)\n",
    "$$\n",
    "and thus\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "x^*&=B^{-1}y^*\\\\\n",
    "&=\\frac{1}{n}B^{-1}\\mathbb{1}\\mathbb{1}^TBx(0)\n",
    "\\end{array}\n",
    "$$\n",
    "by straightforward calculations we get that\n",
    "$$\n",
    "x_i^*=\\frac{\\sum_{i=1}^n\\beta_ix_i(0)}{n\\beta_i}\n",
    "$$\n",
    "with the invariant $\\beta_ix^*_i=\\beta_jx^*_j$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast averaging\n",
    "For any linear process convergence speed is determined by the eigenvalue with the largest absolute value. The only remark here is that in stochastic processes eigenspace corresponding to $1$ does not change and so does not influence convergence, while other eigenvectors are shrinked according to their corresponding eigenvalues. So for the process (1) convergence speed is determined by the second largest eigenvalue of $I-\\gamma\\mathcal{L}$ or the second lowest eigenvalue of $\\mathcal{L}$ that is usually referred to as <i>algebraic connectivity</i> of a graph. It happens that for a directed graphs this value can be minimized via convex optimization problem.\n",
    "\n",
    "First for undirected graph there is an identity for a laplacian\n",
    "$$\n",
    "\\mathcal{L}=CWC^T\n",
    "$$\n",
    "where $W=\\diag\\{a_1, \\ldots, a_m\\}$ - edge weights; $C$ is the incidence matrix of a graph: $C_{ij}$ is $-1$ if $j$ is outcoming edge of $i$, $1$ if $j$ is incoming to $i$ and 0 otherwise. Note that definition of $B$ requires direction of an edge, but surprisingly the matrix $CWC^T$ is defined uniquely independent of this choice. Minimization of a largest absolute value of matrix $A$ can be described as an semi-definite programmin (SDP) problem\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\mbox{minimize } & s\\\\\n",
    "\\mbox{subject to } & -sI\\preceq A\\preceq sI\n",
    "\\end{array}\n",
    "$$\n",
    "Both inequalities are matrix inequalities: $A\\preceq B$ means that for any $x^T(B-A)x\\leq 0$ for all $x$ which is equivalent all eigenvalues of $B-A$ being non-negative. So left inequality bounds all eigenvalues of $A$ to be at least $-s$ and right inequality bounds all eigenvalues of $A$ to be at most $s$, by minimizing $s$ under these conditions we get $A$ with its maximum by absolute value eigenvalue being minimized.\n",
    "\n",
    "Now applying that concept to our case we just need to get rid of the eigenspace of zero which we know has the form of $\\frac{1}{n}\\mathbb{1}\\mathbb{1}^T$. Finally we get the following problem\n",
    "$$\n",
    "\\begin{array}{rl}\n",
    "\\mbox{minimize } & s\\\\\n",
    "\\mbox{subject to } & -sI\\preceq I-\\frac{1}{n}\\mathbb{1}\\mathbb{1}^T-CWC^T\\preceq sI.\n",
    "\\end{array}\n",
    "$$\n",
    "This problem can be solved via any SDP solver, for example [cvxpy](https://www.cvxpy.org/) is capable to solve it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
